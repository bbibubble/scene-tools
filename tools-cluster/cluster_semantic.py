import re
import json
import numpy as np
from collections import defaultdict, Counter
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
import hdbscan

# ============================================================
# 0. é…ç½®
# ============================================================

MIN_TOOLS_PER_SCENE = 2
MIN_TOOLS_PER_SERVICE = 3
MIN_SERVICE_SUPPORT = 5

# HDBSCAN å‚æ•°
HDBSCAN_MIN_CLUSTER_SIZE = 2   # ç°‡æœ€å°‘å·¥å…·æ•°
HDBSCAN_MIN_SAMPLES = 1        # è¶Šå°è¶Šå®½æ¾ï¼Œå™ªå£°ç‚¹è¶Šå°‘
# ç¼–ç æ¨¡å‹
EMBED_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

STOP_TOOLS = {
    "Finish", "give_up_and_restart", "confirm_for_auth",
    "getuser_for_auth", "getuserlist_for_auth"
}
TOOL_PATTERN = re.compile(r"^[a-z][a-z0-9_]+_for_[a-z0-9_]+$", re.IGNORECASE)


# ============================================================
# 1. å·¥å…·è§£æ
# ============================================================

def valid_tool(tool: str) -> bool:
    return tool not in STOP_TOOLS and TOOL_PATTERN.match(tool) is not None


def tool_service(tool: str) -> str:
    return tool.split("_for_")[-1]


def tool_to_text(tool: str) -> str:
    """æŠŠå·¥å…·åè½¬æˆå¯è¯»æ–‡æœ¬ç”¨äºç¼–ç ï¼šä¸‹åˆ’çº¿æ›¿æ¢æˆç©ºæ ¼"""
    return tool.replace("_", " ")


def extract_tool_chain(sample):
    tools = []
    for turn in sample.get("conversations", []):
        if turn.get("from") == "assistant":
            m = re.search(r"Action:\s*([a-zA-Z0-9_]+)", turn.get("value", ""))
            if m:
                t = m.group(1)
                if valid_tool(t):
                    tools.append(t)
    return list(dict.fromkeys(tools))


def load_simple_scenes():
    dataset = load_dataset("Yhyu13/ToolBench_toolllama_G123_dfs", split="train")
    scenes = []
    for sample in dataset:
        tools = extract_tool_chain(sample)
        if len(tools) >= MIN_TOOLS_PER_SCENE:
            scenes.append(tools)
    print(f"âœ“ Parsed {len(scenes)} scenes")
    return scenes


# ============================================================
# 2. Service èšåˆ
# ============================================================

def build_service_profiles(scenes):
    service_data = defaultdict(lambda: {"tools": set(), "scene_count": 0, "scenes_using": []})
    for idx, scene in enumerate(scenes):
        seen_services = defaultdict(list)
        for tool in scene:
            seen_services[tool_service(tool)].append(tool)
        for service, tools in seen_services.items():
            service_data[service]["tools"].update(tools)
            service_data[service]["scene_count"] += 1
            service_data[service]["scenes_using"].append(idx)

    filtered = {
        s: d for s, d in service_data.items()
        if len(d["tools"]) >= MIN_TOOLS_PER_SERVICE
        and d["scene_count"] >= MIN_SERVICE_SUPPORT
    }
    print(f"âœ“ {len(filtered)} services after filtering (dropped {len(service_data)-len(filtered)})")
    return filtered


# ============================================================
# 3. è¯­ä¹‰ç¼–ç 
# ============================================================

def embed_tools(all_tools: list, model) -> np.ndarray:
    """æŠŠå·¥å…·ååˆ—è¡¨ç¼–ç æˆçŸ©é˜µï¼Œè¡Œå¯¹åº”å·¥å…·"""
    texts = [tool_to_text(t) for t in all_tools]
    embeddings = model.encode(texts, batch_size=256, show_progress_bar=True, normalize_embeddings=True)
    return embeddings


# ============================================================
# 4. HDBSCAN è¯­ä¹‰èšç±»
# ============================================================

def cluster_tools_semantic(tools: list, embeddings: np.ndarray) -> dict[str, list]:
    """
    å¯¹ä¸€ä¸ª service å†…çš„å·¥å…·åš HDBSCAN è¯­ä¹‰èšç±»ã€‚
    å¤§ç°‡ï¼ˆ>15ä¸ªå·¥å…·ï¼‰è‡ªåŠ¨æé«˜ min_cluster_sizeï¼Œå¼ºåˆ¶åˆ‡å¾—æ›´ç»†ã€‚
    å™ªå£°ç‚¹ï¼ˆlabel=-1ï¼‰å¦‚æœæ•°é‡å¤Ÿå¤šåˆ™å•ç‹¬æˆç°‡ï¼Œå¦åˆ™ä¸¢å¼ƒã€‚
    """
    if len(tools) <= HDBSCAN_MIN_CLUSTER_SIZE:
        return {"0": tools}

    # å¤§ç°‡åŠ¨æ€è°ƒæ•´ min_cluster_sizeï¼Œè®© HDBSCAN åˆ‡å¾—æ›´ç»†
    n = len(tools)
    if n > 15:
        adaptive_min_cluster = max(2, n // 5)   # æ¯ç°‡çº¦å æ€»æ•°çš„20%
    else:
        adaptive_min_cluster = HDBSCAN_MIN_CLUSTER_SIZE

    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=adaptive_min_cluster,
        min_samples=HDBSCAN_MIN_SAMPLES,
        metric="euclidean",
        cluster_selection_method="leaf",  # leaf æ¯” eom åˆ‡å¾—æ›´ç»†
    )
    labels = clusterer.fit_predict(embeddings)

    groups = defaultdict(list)
    for tool, label in zip(tools, labels):
        groups[str(label)].append(tool)

    # å™ªå£°ç‚¹å•ç‹¬å¤„ç†ï¼šå¤Ÿå¤šå°±ä¿ç•™ï¼Œå¦åˆ™ä¸¢å¼ƒ
    noise = groups.pop("-1", [])
    if len(noise) >= 2:
        groups["noise"] = noise

    # å¦‚æœå…¨éƒ¨æ˜¯å™ªå£°ï¼ˆèšç±»å®Œå…¨å¤±è´¥ï¼‰ï¼Œé€€å›æ•´ä½“ä½œä¸ºä¸€ä¸ªç°‡
    if not groups:
        return {"0": tools}

    return dict(groups)


# ============================================================
# 5. Fallbackï¼šHDBSCAN æœªåˆ‡åˆ†æ—¶æŒ‰åŠŸèƒ½å‰ç¼€åšå…³é”®è¯åˆ‡åˆ†
# ============================================================

FALLBACK_FUNC_GROUPS = [
    # é€šç”¨æ“ä½œ
    ["search", "find", "lookup", "query"],
    ["by_class", "by_type", "by_race", "by_faction", "by_quality", "by_set", "by_category"],
    ["get", "single", "detail", "info", "about"],
    ["all", "list", "index", "catalog"],
    ["create", "add", "post", "insert"],
    ["update", "edit", "modify"],
    ["delete", "remove"],
    ["forecast", "predict", "future"],
    ["current", "live", "realtime", "now"],
    ["history", "historical", "past", "stats"],
    ["top", "trending", "popular", "rank", "best"],
    # é‡‘èæŠ€æœ¯æŒ‡æ ‡ï¼ˆç§»åŠ¨å‡çº¿ç±»ï¼‰
    ["ema", "sma", "wma", "dema", "tema", "trima", "t3ma", "mama", "ma_"],
    # é‡‘èæŠ€æœ¯æŒ‡æ ‡ï¼ˆåŠ¨é‡/éœ‡è¡ç±»ï¼‰
    ["rsi", "cci", "mom", "roc", "rocr", "cmo", "crsi", "mfi", "willr", "ultosc", "coppock"],
    # é‡‘èæŠ€æœ¯æŒ‡æ ‡ï¼ˆè¶‹åŠ¿ç±»ï¼‰
    ["adx", "aroon", "sar", "apo", "ppo", "macd", "dmi", "adosc", "obv"],
    # é‡‘èæŠ€æœ¯æŒ‡æ ‡ï¼ˆç»Ÿè®¡/æ•°å­¦ç±»ï¼‰
    ["stddev", "var", "beta", "correl", "linearreg", "sqrt", "ln", "ceil", "floor",
     "min_", "max_", "avg", "sum", "sub", "div", "midpoint", "midprice", "medprice",
     "avgprice", "wclprice", "minmax", "minus_di", "percent_b", "ht_"],
    # é‡‘èåŸºæœ¬é¢
    ["earnings", "balance_sheet", "dividends", "eps", "growth", "institutional",
     "sustainability", "recommendations", "analyst", "profile", "statistics", "risk"],
    # é‡‘èå¸‚åœºæ•°æ®
    ["market_movers", "quote", "real_time_price", "time_series", "options",
     "ipo", "composition", "exchanges", "crypto_exchanges", "currency_conversion",
     "earliest_timestamp", "symbol_search", "logo"],
    # ä½“è‚²èµ›äº‹ç±»
    ["match", "fixture", "result", "standing", "score", "league", "season"],
    ["player", "team", "coach", "squad", "transfer"],
    ["live", "odds", "lineup", "event"],
    # ç”¨æˆ·å/è´¦å·æ£€æŸ¥ç±»
    ["instagram", "twitter", "facebook", "tiktok", "snapchat", "reddit",
     "youtube", "twitch", "github", "pinterest", "tumblr", "telegram"],
]

def fallback_keyword_split(tools: list) -> dict[str, list]:
    """
    HDBSCAN æœªèƒ½åˆ‡åˆ†æ—¶çš„å…œåº•æ–¹æ¡ˆï¼šæŒ‰å·¥å…·ååŠŸèƒ½å…³é”®è¯åˆ†ç»„ã€‚
    åŒ¹é…ä¸åˆ°ä»»ä½•ç»„çš„å·¥å…·å½’å…¥ miscã€‚
    """
    groups = defaultdict(list)
    for tool in tools:
        tool_lower = tool.lower()
        matched = False
        for group_kws in FALLBACK_FUNC_GROUPS:
            if any(kw in tool_lower for kw in group_kws):
                groups[group_kws[0]].append(tool)  # ç”¨ç¬¬ä¸€ä¸ªå…³é”®è¯ä½œä¸ºç»„å
                matched = True
                break
        if not matched:
            groups["misc"].append(tool)

    # è¿‡æ»¤å¤ªå°çš„ç»„ï¼ˆåˆå¹¶åˆ° miscï¼‰
    result = {}
    misc = list(groups.get("misc", []))
    for label, group_tools in groups.items():
        if label == "misc":
            continue
        if len(group_tools) < 2:
            misc.extend(group_tools)
        else:
            result[label] = group_tools
    if misc:
        result["misc"] = misc

    return result if len(result) > 1 else {"0": tools}  # åˆ‡ä¸å¼€å°±åŸæ ·è¿”å›


# ============================================================
# 6. ç”Ÿæˆ theme æè¿°ï¼ˆç”¨ service å + ç°‡å†…é«˜é¢‘è¯ï¼‰
# ============================================================

def infer_cluster_theme(service: str, tools: list) -> str:
    """ä» service åå’Œå·¥å…·åæå–è¯­ä¹‰åŒ–ä¸»é¢˜æè¿°"""
    # æå–å·¥å…·ååŠŸèƒ½éƒ¨åˆ†ï¼ˆ_for_ å‰çš„è¯ï¼‰çš„é«˜é¢‘è¯
    word_counter = Counter()
    for tool in tools:
        func_part = tool.split("_for_")[0]
        words = func_part.split("_")
        for w in words:
            if len(w) > 2:  # è¿‡æ»¤å¤ªçŸ­çš„è¯
                word_counter[w] += 1

    # å–å‰2ä¸ªé«˜é¢‘è¯ä½œä¸ºåŠŸèƒ½æè¿°
    top_words = [w for w, _ in word_counter.most_common(2)]
    service_readable = service.replace("_", " ")
    func_desc = " ".join(top_words) if top_words else "general"

    return f"{func_desc} {service_readable}".strip()


# ============================================================
# 7. ä¸»æµç¨‹ï¼šæ„å»ºç»†ç²’åº¦ compounds
# ============================================================

def build_semantic_compounds(scenes, service_profiles, model):
    # æ”¶é›†æ‰€æœ‰å·¥å…· + å¯¹åº” service
    all_tools_ordered = []
    tool_to_service = {}
    for service, profile in service_profiles.items():
        for tool in profile["tools"]:
            if tool not in tool_to_service:
                all_tools_ordered.append(tool)
                tool_to_service[tool] = service

    print(f"\nğŸ”¢ Encoding {len(all_tools_ordered)} tools...")
    all_embeddings = embed_tools(all_tools_ordered, model)
    tool_to_embedding = {t: all_embeddings[i] for i, t in enumerate(all_tools_ordered)}

    compounds = []
    compound_id = 0

    for service, profile in service_profiles.items():
        service_tools = sorted(profile["tools"])
        scene_indices = set(profile["scenes_using"])

        if len(service_tools) < 2:
            continue

        # å–è¯¥ service å·¥å…·çš„ embedding å­çŸ©é˜µ
        service_embeddings = np.stack([tool_to_embedding[t] for t in service_tools])

        # HDBSCAN èšç±»
        clusters = cluster_tools_semantic(service_tools, service_embeddings)

        # å¦‚æœæ•´ä¸ª service åªäº§å‡º1ä¸ªç°‡ä¸”å·¥å…·æ•°>6ï¼Œfallback åˆ°å…³é”®è¯åˆ‡åˆ†
        if len(clusters) == 1 and len(service_tools) > 6:
            only_label = list(clusters.keys())[0]
            if only_label != "-1":
                fallback = fallback_keyword_split(service_tools)
                if len(fallback) > 1:
                    clusters = fallback

        # å¯¹ä»ç„¶è¶…è¿‡20ä¸ªå·¥å…·çš„ç°‡ï¼Œå¼ºåˆ¶å†åšä¸€æ¬¡fallbackå…³é”®è¯åˆ‡åˆ†
        expanded_clusters = {}
        for label, cluster_tools in clusters.items():
            if len(cluster_tools) > 20:
                sub = fallback_keyword_split(cluster_tools)
                if len(sub) > 1:
                    for sub_label, sub_tools in sub.items():
                        expanded_clusters[f"{label}_{sub_label}"] = sub_tools
                else:
                    expanded_clusters[label] = cluster_tools
            else:
                expanded_clusters[label] = cluster_tools
        clusters = expanded_clusters

        for cluster_label, cluster_tools in clusters.items():
            # å™ªå£°ç‚¹æˆ–è¿‡å°çš„ç°‡ç›´æ¥è·³è¿‡
            if len(cluster_tools) < 2:
                continue
            # è¶…è¿‡20ä¸ªå·¥å…·çš„ç°‡æ ‡è®°ä¸ºlargeï¼Œä¿ç•™è¿›test_large.json
            if len(cluster_tools) > 20:
                print(f"  ğŸ“¦ è¶…å¤§ç°‡ {service}/{cluster_label}ï¼š{len(cluster_tools)} ä¸ªå·¥å…· â†’ test_large.json")

            # è®¡ç®— support
            chunk_set = set(cluster_tools)
            support = sum(
                1 for idx in scene_indices
                if any(t in chunk_set for t in scenes[idx])
            )

            theme_desc = infer_cluster_theme(service, cluster_tools)
            compound_id += 1

            compounds.append({
                "compound_id": f"compound_{compound_id:05d}",
                "theme": theme_desc,
                "service": service,
                "cluster_label": cluster_label,
                "tools": sorted(cluster_tools),
                "num_tools": len(cluster_tools),
                "num_simple_scenes": support,
            })

    # æŒ‰ support é™åºæ’åˆ—
    compounds.sort(key=lambda x: x["num_simple_scenes"], reverse=True)
    return compounds


# ============================================================
# 7. è½¬æ¢ä¸º test.json
# ============================================================

def convert_to_test_json(compounds: list) -> list:
    return [
        {
            "theme": c["theme"],
            "tools": c["tools"],
            "num_tools": c["num_tools"],
        }
        for c in compounds
    ]


# ============================================================
# 8. ä¸»å…¥å£
# ============================================================

def main():
    print("=" * 60)
    print("è¯­ä¹‰èšç±»ï¼šsentence-transformers + HDBSCAN")
    print("=" * 60)

    scenes = load_simple_scenes()

    print("\n[Stage 1] Building service profiles...")
    service_profiles = build_service_profiles(scenes)

    print("\n[Stage 2] Loading embedding model...")
    model = SentenceTransformer(EMBED_MODEL)

    print("\n[Stage 3] Semantic clustering...")
    compounds = build_semantic_compounds(scenes, service_profiles, model)

    print(f"\nâœ“ Generated {len(compounds)} compounds")

    # åˆ†å¸ƒç»Ÿè®¡
    tool_dist = Counter(c["num_tools"] for c in compounds)
    print("\nğŸ“Š Tools-per-compound distribution:")
    for n in sorted(tool_dist):
        print(f"  {n} tools: {tool_dist[n]} compounds")

    # ä¿å­˜
    with open("toolbench_semantic_compounds.json", "w", encoding="utf-8") as f:
        json.dump(compounds, f, ensure_ascii=False, indent=2)
    print("\nâœ“ Saved to toolbench_semantic_compounds.json")

    tasks = convert_to_test_json(compounds)
    tasks_small = [t for t in tasks if len(t["tools"]) <= 10]
    tasks_large = [t for t in tasks if len(t["tools"]) > 10]

    with open("test_small.json", "w", encoding="utf-8") as f:
        json.dump(tasks_small, f, ensure_ascii=False, indent=2)
    print(f"âœ“ Saved {len(tasks_small)} tasks (â‰¤10 tools) to test_small.json")

    with open("test_large.json", "w", encoding="utf-8") as f:
        json.dump(tasks_large, f, ensure_ascii=False, indent=2)
    print(f"âœ“ Saved {len(tasks_large)} tasks (>10 tools) to test_large.json")

    # æ ·ä¾‹
    with open("sample_semantic.json", "w", encoding="utf-8") as f:
        json.dump(compounds[:5], f, ensure_ascii=False, indent=2)
    print("âœ“ Saved sample to sample_semantic.json")


if __name__ == "__main__":
    main()
